import streamlit as st
import feedparser
import os
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv
import urllib.parse
from datetime import datetime
from notion_client import Client
import schedule
import time
import threading

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
notion_api_key = os.getenv("NOTION_API_KEY")
notion_db_id = os.getenv("NOTION_DATABASE_ID")

# í•„ìˆ˜ í‚¤ ê²€ì¦
if not openai_api_key:
    st.error("â›” OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

if not notion_api_key or not notion_db_id:
    st.error("â›” Notion ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    base_url="https://gms.ssafy.io/gmsapi/api.openai.com/v1",
    api_key=openai_api_key
)
notion = Client(auth=notion_api_key)
MODEL_NAME = "gpt-5-nano"

# 2. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤

def get_real_url(rss_link):
    """Google RSS ë§í¬ì˜ ì‹¤ì œ ì£¼ì†Œë¥¼ ì¶”ì  (ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ê²°)"""
    try:
        res = requests.head(rss_link, allow_redirects=True, timeout=5)
        return res.url
    except:
        return rss_link

def crawl_article(url):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ (ìŠ¤í¬ë˜í•‘)"""
    try:
        # ë´‡ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=5)
        
        if response.status_code != 200:
            return "ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì ‘ê·¼ ì œí•œ)"

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ëŒ€ë¶€ë¶„ì˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ëŠ” <p> íƒœê·¸ì— ë³¸ë¬¸ì´ ìˆìŒ
        paragraphs = soup.find_all('p')
        content = " ".join([p.get_text() for p in paragraphs])
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìˆ˜ì§‘ ì‹¤íŒ¨ë¡œ ê°„ì£¼
        if len(content) < 50:
            return "ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë‚´ìš© ì—†ìŒ)"
            
        return content[:3000] # LLM ì…ë ¥ ì œí•œì„ ê³ ë ¤í•´ 3000ìê¹Œì§€ë§Œ
    except Exception as e:
        return f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}"

def fetch_google_news(keyword):
    """RSS ìˆ˜ì§‘ + ë³¸ë¬¸ í¬ë¡¤ë§ í†µí•©"""
    encoded_keyword = urllib.parse.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)
    
    news_items = []
    # ì†ë„ë¥¼ ìœ„í•´ ìƒìœ„ 2ê°œë§Œ ìˆ˜ì§‘
    for entry in feed.entries[:2]:
        real_url = get_real_url(entry.link)
        content = crawl_article(real_url)
        
        news_items.append({
            "title": entry.title,
            "link": real_url,
            "pubDate": entry.published,
            "content": content
        })
    return news_items

def summarize_news(news_data, query):
    """ë³¸ë¬¸ ë‚´ìš©ì„ í¬í•¨í•œ ê³ í’ˆì§ˆ ìš”ì•½"""
    prompt_text = ""
    for idx, item in enumerate(news_data, 1):
        prompt_text += f"\n[ê¸°ì‚¬ {idx}: {item['title']}]\në³¸ë¬¸ë‚´ìš©: {item['content']}\n"

    system_prompt = f"ì‚¬ìš©ìê°€ '{query}'ì— ëŒ€í•´ ê²€ìƒ‰í–ˆì–´. ìœ„ ê¸°ì‚¬ë“¤ì˜ 'ë³¸ë¬¸ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ì •ë³´ë¥¼ ì¢…í•©í•´ì„œ 3ì¤„ë¡œ ê¹”ë”í•˜ê²Œ ìš”ì•½í•´ì¤˜."
    
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt_text}
        ]
    )
    return response.choices[0].message.content

def save_to_notion(query, summary, link=None):
    """Notion ì €ì¥"""
    try:
        notion.pages.create(
            parent={"database_id": notion_db_id},
            properties={
                "ê²€ìƒ‰ì–´": {"title": [{"text": {"content": query}}]},
                "ìš”ì•½ë‚´ìš©": {"rich_text": [{"text": {"content": summary}}]},
                "ë‚ ì§œ": {"date": {"start": datetime.now().isoformat()}},
                "ë§í¬": {"url": link if link else None}
            }
        )
        print(f"[Log] Notion saved: {query}")
        return True
    except Exception as e:
        print(f"[Error] Notion save failed: {e}")
        return False

# 3. ìë™í™” ìŠ¤ì¼€ì¤„ë§ ë¡œì§

def scheduled_job():
    """ë§¤ì¼ ì‹¤í–‰ë  ìë™ ìˆ˜ì§‘ ì‘ì—…"""
    print("â° ìë™ ìˆ˜ì§‘ ì‹œì‘...")
    target_keyword = "ìµœì‹  AI ê¸°ìˆ " # ìë™ ìˆ˜ì§‘í•  ì£¼ì œ
    items = fetch_google_news(target_keyword)
    if items:
        summary = summarize_news(items, target_keyword)
        save_to_notion(f"[ìë™] {target_keyword}", summary, items[0]['link'])
    print("âœ… ìë™ ìˆ˜ì§‘ ì™„ë£Œ")

def start_scheduler():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 'ë§¤ ë¶„' ë§ˆë‹¤ ì‹¤í–‰ (ë°°í¬ ì‹œì—” .every().day.at("09:00") ë“±ìœ¼ë¡œ ë³€ê²½)
    schedule.every().day.at("09:00").do(scheduled_job) 
    
    while True:
        schedule.run_pending()
        time.sleep(1)

# Streamlit ì‹¤í–‰ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤ë ˆë“œ ì‹œì‘ (í•œ ë²ˆë§Œ)
if "scheduler_started" not in st.session_state:
    t = threading.Thread(target=start_scheduler, daemon=True)
    t.start()
    st.session_state.scheduler_started = True

# 4. Streamlit UI (ê¸°ì¡´ê³¼ ë™ì¼í•˜ë˜ ê²€ìƒ‰ ì‹œ í¬ë¡¤ë§ ì ìš©)

st.title("ğŸ“° AI ë‰´ìŠ¤ ë´‡ (í¬ë¡¤ë§ & ìë™í™”)")

with st.sidebar:
    st.header("ì„¤ì • ë° ì •ë³´")
    st.markdown("[ğŸ‘‰ Notion ë°”ë¡œê°€ê¸°](https://www.notion.so)")
    st.info("ì˜¤ì „ 9ì‹œë§ˆë‹¤ 'ìµœì‹  AI ê¸°ìˆ ' ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ê²€ìƒ‰í•  ë‰´ìŠ¤ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # ê°„ë‹¨í•œ ì˜ë„ íŒë³„ (ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
        if prompt in ["ì•ˆë…•", "ë°˜ê°€ì›Œ"]:
            full_response = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        else:
            message_placeholder.markdown("ğŸ•µï¸ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì½ê³  ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë ¤ìš”)")
            
            items = fetch_google_news(prompt)
            if items:
                summary = summarize_news(items, prompt)
                save_to_notion(prompt, summary, items[0]['link'])
                
                full_response = f"**['{prompt}' ì‹¬ì¸µ ìš”ì•½]**\n\n{summary}\n\n**ì¶œì²˜:**"
                for item in items:
                    full_response += f"\n- [{item['title']}]({item['link']})"
            else:
                full_response = "ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ì ‘ê·¼ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤."

        message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})